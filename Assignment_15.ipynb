{"cells":[{"cell_type":"markdown","metadata":{"id":"6BUII66XMjYH"},"source":["Q1.  **Deep Learning.**\n","\n","    1.  **Build a DNN with five hidden layers of 100 neurons each, He\n","        initialization, and the ELU activation function.**\n","\n","    2.  **Using Adam optimization and early stopping, try training it on\n","        MNIST but only on digits 0 to 4, as we will use transfer\n","        learning for digits 5 to 9 in the next exercise. You will need a\n","        softmax output layer with five neurons, and as always make sure\n","        to save checkpoints at regular intervals and save the final\n","        model so you can reuse it later.**\n","\n","    3.  **Tune the hyperparameters using cross-validation and see what\n","        precision you can achieve.**\n","\n","    4.  **Now try adding Batch Normalization and compare the learning\n","        curves: is it converging faster than before? Does it produce a\n","        better model?**\n","\n","    5.  **Is the model overfitting the training set? Try adding dropout\n","        to every layer and try again. Does it help?**\n","\n","> **a. Here's how you can build a DNN with five hidden layers of 100\n","> neurons each, using He initialization and the ELU activation\n","> function:**\n",">\n","> import tensorflow as tf\n",">\n","> from tensorflow import keras\n",">\n","> \\# Load MNIST dataset\n",">\n","> (X_train, y_train), (X_test, y_test) =\n","> keras.datasets.mnist.load_data()\n",">\n","> \\# Filter digits 0 to 4\n",">\n","> train_mask = y_train \\< 5\n",">\n","> test_mask = y_test \\< 5\n",">\n","> X_train, y_train = X_train\\[train_mask\\], y_train\\[train_mask\\]\n",">\n","> X_test, y_test = X_test\\[test_mask\\], y_test\\[test_mask\\]\n",">\n","> \\# Preprocess the data\n",">\n","> X_train = X_train.reshape(-1, 28\\*28) / 255.0\n",">\n","> X_test = X_test.reshape(-1, 28\\*28) / 255.0\n",">\n","> y_train = keras.utils.to_categorical(y_train, num_classes=5)\n",">\n","> y_test = keras.utils.to_categorical(y_test, num_classes=5)\n",">\n","> \\# Build the model\n",">\n","> model = keras.models.Sequential(\\[\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\", input_shape=(28\\*28,)),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dense(5, activation=\"softmax\")\n",">\n","> \\])\n",">\n","> \\# Compile the model\n",">\n","> model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n","> metrics=\\[\"accuracy\"\\])\n",">\n","> \\# Define callbacks\n",">\n","> callbacks = \\[\n",">\n","> keras.callbacks.EarlyStopping(patience=10),\n",">\n","> keras.callbacks.ModelCheckpoint(\"mnist_dnn.h5\", save_best_only=True)\n",">\n","> \\]\n",">\n","> \\# Train the model\n",">\n","> history = model.fit(X_train, y_train, epochs=100,\n","> validation_data=(X_test, y_test), callbacks=callbacks)\n",">\n","> \\# Save the final model\n",">\n","> model.save(\"mnist_dnn_final.h5\")\n",">\n","> **b.** **The code above trains the model on the MNIST dataset,\n","> considering only digits 0 to 4. It uses the Adam optimizer and early\n","> stopping to prevent overfitting. The output layer has five neurons\n","> with softmax activation**.\n",">\n","> **c.** **To tune the hyperparameters, you can use cross-validation.\n","> You can vary parameters like the number of hidden layers, the number\n","> of neurons per layer, the learning rate, or the activation function to\n","> find the best combination that yields the highest precision.**\n",">\n","> from sklearn.model_selection import GridSearchCV\n",">\n","> \\# Define the model\n",">\n","> def create_model():\n",">\n","> model = keras.models.Sequential(\\[\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\", input_shape=(28\\*28,)),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dense(5, activation=\"softmax\")\n",">\n","> \\])\n",">\n","> model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n","> metrics=\\[\"accuracy\"\\])\n",">\n","> return model\n",">\n","> \\# Create the model\n",">\n","> model =\n","> keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_model)\n",">\n","> \\# Define the hyperparameters to tune\n",">\n","> param_grid = {\n",">\n","> \"epochs\": \\[50, 100, 150\\],\n",">\n","> \"batch_size\": \\[32, 64, 128\\],\n",">\n","> \"learning_rate\": \\[0.001, 0.01, 0.1\\],\n",">\n","> \"\n",">\n","> hidden_layers\": \\[2, 3, 4, 5\\],\n",">\n","> \"neurons_per_layer\": \\[50, 100, 200\\]\n",">\n","> }\n",">\n","> \\# Perform grid search cross-validation\n",">\n","> grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",">\n","> grid_result = grid.fit(X_train, y_train)\n",">\n","> \\# Get the best precision achieved\n",">\n","> best_precision = grid_result.best_score\\_\n",">\n","> print(\"Best Precision: %.4f\" % (best_precision))\n",">\n","> **d.** **To add Batch Normalization and compare the learning curves,\n","> you can modify the model architecture as follows**:\n",">\n","> model = keras.models.Sequential(\\[\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\", input_shape=(28\\*28,)),\n",">\n","> keras.layers.BatchNormalization(),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.BatchNormalization(),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.BatchNormalization(),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.BatchNormalization(),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.BatchNormalization(),\n",">\n","> keras.layers.Dense(5, activation=\"softmax\")\n",">\n","> \\])\n",">\n","> You can train and compare the learning curves with and without Batch\n","> Normalization to see if it converges faster and produces a better\n","> model.\n",">\n","> **e. To check if the model is overfitting, you can add dropout to\n","> every layer. Dropout randomly sets a fraction of the input units to 0\n","> at each update during training, which helps prevent overfitting.**\n",">\n","> model = keras.models.Sequential(\\[\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\", input_shape=(28\\*28,)),\n",">\n","> keras.layers.Dropout(0.5),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dropout(0.5),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dropout(0.5),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dropout(0.5),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dropout(0.5),\n",">\n","> keras.layers.Dense(5, activation=\"softmax\")\n",">\n","> \\])\n",">\n","> By adding dropout to every layer, you can train the model again and\n","> observe if it helps reduce overfitting.\n","\n","Q2.  **Transfer learning.**\n","\n","    1.  **Create a new DNN that reuses all the pretrained hidden layers\n","        of the previous model, freezes them, and replaces the softmax\n","        output layer with a new one.**\n","\n","    2.  **Train this new DNN on digits 5 to 9, using only 100 images per\n","        digit, and time how long it takes. Despite this small number of\n","        examples, can you achieve high precision?**\n","\n","    3.  **Try caching the frozen layers, and train the model again: how\n","        much faster is it now?**\n","\n","    4.  **Try again reusing just four hidden layers instead of five. Can\n","        you achieve a higher precision?**\n","\n","    5.  **Now unfreeze the top two hidden layers and continue training:\n","        can you get the model to perform even better?**\n","\n","> **a. To create a new DNN that reuses the pretrained hidden layers of\n","> the previous model, freezes them, and replaces the softmax output\n","> layer with a new one, you can follow these steps:**\n",">\n","> import tensorflow as tf\n",">\n","> from tensorflow import keras\n",">\n","> \\# Load the previously trained model\n",">\n","> pretrained_model = keras.models.load_model(\"mnist_dnn_final.h5\")\n",">\n","> \\# Freeze the pretrained layers\n",">\n","> for layer in pretrained_model.layers:\n",">\n","> layer.trainable = False\n",">\n","> \\# Remove the last softmax layer\n",">\n","> pretrained_model.pop()\n",">\n","> \\# Add a new softmax output layer\n",">\n","> pretrained_model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",">\n","> \\# Compile the model\n",">\n","> pretrained_model.compile(loss=\"categorical_crossentropy\",\n","> optimizer=\"adam\", metrics=\\[\"accuracy\"\\])\n",">\n","> **b. To train the new DNN on digits 5 to 9 using a small number of\n","> images per digit, you can modify the MNIST dataset accordingly and\n","> train the model. Note that with a small number of images, achieving\n","> high precision may be challenging, but it's still worth trying.**\n",">\n","> \\# Load MNIST dataset\n",">\n","> (X_train, y_train), (X_test, y_test) =\n","> keras.datasets.mnist.load_data()\n",">\n","> \\# Filter digits 5 to 9\n",">\n","> train_mask = y_train \\>= 5\n",">\n","> test_mask = y_test \\>= 5\n",">\n","> X_train, y_train = X_train\\[train_mask\\], y_train\\[train_mask\\]\n",">\n","> X_test, y_test = X_test\\[test_mask\\], y_test\\[test_mask\\]\n",">\n","> \\# Limit the number of images per digit\n",">\n","> num_images_per_digit = 100\n",">\n","> X_train = X_train\\[:num_images_per_digit\\]\n",">\n","> y_train = y_train\\[:num_images_per_digit\\]\n",">\n","> \\# Preprocess the data\n",">\n","> X_train = X_train.reshape(-1, 28\\*28) / 255.0\n",">\n","> X_test = X_test.reshape(-1, 28\\*28) / 255.0\n",">\n","> y_train = keras.utils.to_categorical(y_train - 5, num_classes=5)\n",">\n","> y_test = keras.utils.to_categorical(y_test - 5, num_classes=5)\n",">\n","> \\# Train the model\n",">\n","> history = pretrained_model.fit(X_train, y_train, epochs=100,\n","> validation_data=(X_test, y_test))\n",">\n","> **c. To cache the frozen layers and train the model again, you can\n","> utilize the \\`cache\\` argument in the \\`fit\\` method. This can speed\n","> up the training process by avoiding unnecessary computations for the\n","> frozen layers.**\n",">\n","> \\# Train the model with caching\n",">\n","> history = pretrained_model.fit(X_train, y_train, epochs=100,\n","> validation_data=(X_test, y_test), cache=True)\n",">\n","> The speedup achieved by caching the frozen layers will depend on the\n","> specific hardware and software configuration.\n",">\n","> **d. To reuse just four hidden layers instead of all five, you can\n","> modify the model architecture by removing one hidden layer before\n","> retraining the model. This allows you to evaluate if using fewer\n","> layers can lead to higher precision.**\n",">\n","> pretrained_model.layers.pop() \\# Remove the last hidden layer\n",">\n","> pretrained_model.compile(loss=\"categorical_crossentropy\",\n","> optimizer=\"adam\", metrics=\\[\"accuracy\"\\])\n",">\n","> \\# Train the model with four hidden layers\n",">\n","> history = pretrained_model.fit(X_train, y_train, epochs=100,\n","> validation_data=(X_test, y_test))\n",">\n","> **e. To unfreeze the top two hidden layers and continue training, you\n","> can set the \\`trainable\\` property of those layers to \\`True\\` before\n","> training the model again.**\n",">\n","> for layer in pretrained_model.layers\\[-2:\\]:\n",">\n","> layer.trainable = True\n",">\n","> pretrained_model.compile(loss=\"categorical_crossentropy\",\n","> optimizer=\"adam\", metrics=\\[\"accuracy\"\\])\n",">\n","> \\# Continue training with unfrozen top two layers\n",">\n","> history = pretrained_model.fit(X_train, y\n",">\n","> \\_train, epochs=100, validation_data=(X_test, y_test))\n",">\n","> By unfreezing the top two hidden layers, the model has the opportunity\n","> to adapt and potentially improve its performance.\n","\n","Q3.  **Pretraining on an auxiliary task.**\n","\n","    1.  **In this exercise you will build a DNN that compares two MNIST\n","        digit images and predicts whether they represent the same digit\n","        or not. Then you will reuse the lower layers of this network to\n","        train an MNIST classifier using very little training data. Start\n","        by building two DNNs (let’s call them DNN A and B), both similar\n","        to the one you built earlier but without the output layer: each\n","        DNN should have five hidden layers of 100 neurons each, He\n","        initialization, and ELU activation. Next, add one more hidden\n","        layer with 10 units on top of both DNNs. To do this, you should\n","        use TensorFlow’s concat() function with axis=1 to concatenate\n","        the outputs of both DNNs for each instance, then feed the result\n","        to the hidden layer. Finally, add an output layer with a single\n","        neuron using the logistic activation function.**\n","\n","    2.  **Split the MNIST training set in two sets: split #1 should\n","        containing 55,000 images, and split #2 should contain contain\n","        5,000 images. Create a function that generates a training batch\n","        where each instance is a pair of MNIST images picked from split\n","        #1. Half of the training instances should be pairs of images\n","        that belong to the same class, while the other half should be\n","        images from different classes. For each pair, the training label\n","        should be 0 if the images are from the same class, or 1 if they\n","        are from different classes.**\n","\n","    3.  **Train the DNN on this training set. For each image pair, you\n","        can simultaneously feed the first image to DNN A and the second\n","        image to DNN B. The whole network will gradually learn to tell\n","        whether two images belong to the same class or not.**\n","\n","    4.  **Now create a new DNN by reusing and freezing the hidden layers\n","        of DNN A and adding a softmax output layer on top with 10\n","        neurons. Train this network on split #2 and see if you can\n","        achieve high performance despite having only 500 images per\n","        class.**\n","\n","> **a. To build two DNNs (DNN A and DNN B) for comparing MNIST digit\n","> images and predicting whether they represent the same digit or not,\n","> you can follow these steps:**\n",">\n","> import tensorflow as tf\n",">\n","> from tensorflow import keras\n",">\n","> \\# DNN A\n",">\n","> dnn_a = keras.models.Sequential(\\[\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\", input_shape=(28\\*28,)),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\")\n",">\n","> \\])\n",">\n","> \\# DNN B\n",">\n","> dnn_b = keras.models.Sequential(\\[\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\", input_shape=(28\\*28,)),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\"),\n",">\n","> keras.layers.Dense(100, activation=\"elu\",\n","> kernel_initializer=\"he_normal\")\n",">\n","> \\])\n",">\n","> \\# Concatenate outputs of DNN A and DNN B\n",">\n","> concat = keras.layers.Concatenate(axis=1)(\\[dnn_a.output,\n","> dnn_b.output\\])\n",">\n","> \\# Add a hidden layer with 10 units\n",">\n","> hidden = keras.layers.Dense(10, activation=\"elu\")(concat)\n",">\n","> \\# Output layer with a single neuron using logistic activation\n",">\n","> output = keras.layers.Dense(1, activation=\"sigmoid\")(hidden)\n",">\n","> \\# Combine DNN A and DNN B into a single model\n",">\n","> model = keras.models.Model(inputs=\\[dnn_a.input, dnn_b.input\\],\n","> outputs=output)\n",">\n","> **b. To generate a training batch with pairs of MNIST images, you can\n","> create a function that randomly selects images from split #1 and\n","> labels them as same class (0) or different classes (1) based on their\n","> true labels.**\n",">\n","> import numpy as np\n",">\n","> def generate_training_batch(images, labels, batch_size):\n",">\n","> num_classes = len(np.unique(labels))\n",">\n","> half_batch_size = batch_size // 2\n",">\n","> \\# Generate same class pairs\n",">\n","> same_class_indices = np.random.randint(0, len(images),\n","> size=half_batch_size)\n",">\n","> same_class_images_1 = images\\[same_class_indices\\]\n",">\n","> same_class_labels_1 = labels\\[same_class_indices\\]\n",">\n","> same_class_images_2 = same_class_images_1.copy()\n",">\n","> same_class_labels_2 = same_class_labels_1.copy()\n",">\n","> \\# Generate different class pairs\n",">\n","> different_class_indices = np.random.randint(0, len(images),\n","> size=half_batch_size)\n",">\n","> different_class_images_1 = images\\[different_class_indices\\]\n",">\n","> different_class_labels_1 = labels\\[different_class_indices\\]\n",">\n","> different_class_indices = np.random.randint(0, len(images),\n","> size=half_batch_size)\n",">\n","> different_class_images_2 = images\\[different_class_indices\\]\n",">\n","> different_class_labels_2 = labels\\[different_class_indices\\]\n",">\n","> \\# Combine same class and different class pairs\n",">\n","> X_1 = np.concatenate(\\[same_class_images_1,\n","> different_class_images_1\\], axis=0)\n",">\n","> X_2 = np.concatenate(\\[same_class_images_2,\n","> different_class_images_2\\], axis=0)\n",">\n","> y = np.concatenate(\\[np.zeros(half_batch_size),\n","> np.ones(half_batch_size)\\], axis=0)\n",">\n","> return \\[X_1, X_2\\], y\n",">\n","> **c. To train the DNN on the generated training set, you can use the**\n",">\n","> \\`generate_training_batch\\` function in a loop and feed the image\n","> pairs to DNN A and DNN B simultaneously.\n",">\n","> \\# Split MNIST training set into split #1 and split #2\n",">\n","> (X_train, y_train), (X_test, y_test) =\n","> keras.datasets.mnist.load_data()\n",">\n","> X_train_1, y_train_1 = X_train\\[:55000\\], y_train\\[:55000\\]\n",">\n","> X_train_2, y_train_2 = X_train\\[55000:\\], y_train\\[55000:\\]\n",">\n","> \\# Preprocess the data\n",">\n","> X_train_1 = X_train_1.reshape(-1, 28\\*28) / 255.0\n",">\n","> X_train_2 = X_train_2.reshape(-1, 28\\*28) / 255.0\n",">\n","> \\# Training loop\n",">\n","> batch_size = 32\n",">\n","> num_batches = len(X_train_1) // batch_size\n",">\n","> for epoch in range(10):\n",">\n","> for batch in range(num_batches):\n",">\n","> X_batch, y_batch = generate_training_batch(X_train_1, y_train_1,\n","> batch_size)\n",">\n","> model.train_on_batch(X_batch, y_batch)\n",">\n","> **d. To create a new DNN by reusing and freezing the hidden layers of\n","> DNN A and adding a softmax output layer on top, you can follow these\n","> steps:**\n",">\n","> \\# Reuse and freeze the hidden layers of DNN A\n",">\n","> dnn_a.trainable = False\n",">\n","> dnn_a\\_outputs = dnn_a.layers\\[-1\\].output\n",">\n","> \\# Add a softmax output layer with 10 neurons\n",">\n","> softmax_layer = keras.layers.Dense(10,\n","> activation=\"softmax\")(dnn_a\\_outputs)\n",">\n","> \\# Create a new model with frozen layers of DNN A and the softmax\n","> output layer\n",">\n","> new_model = keras.models.Model(inputs=dnn_a.input,\n","> outputs=softmax_layer)\n",">\n","> \\# Compile the new model\n",">\n","> new_model.compile(loss=\"sparse_categorical_crossentropy\",\n","> optimizer=\"adam\", metrics=\\[\"accuracy\"\\])\n",">\n","> You can then train the new model on split #2 and evaluate its\n","> performance despite having only 500 images per class.\n",">\n","> \\# Preprocess split #2 data\n",">\n","> X_train_2 = X_train_2.reshape(-1, 28\\*28) / 255.0\n",">\n","> \\# Train the new model on split #2\n",">\n","> new_model.fit(X_train_2, y_train_2, epochs=10,\n","> validation_data=(X_test, y_test))\n",">\n","> Despite having a small number of images per class, transfer learning\n","> from the pretrained DNN A can still enable the new model to achieve a\n","> reasonable performance."],"id":"6BUII66XMjYH"}],"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"provenance":[]}}}