{"cells":[{"cell_type":"markdown","metadata":{"id":"A4NJsgr6LLTu"},"source":["Q1.  **Write the Python code to implement a single neuron.**\n","\n","> Sure! **Here's an example of Python code that implements a single\n","> neuron:**\n",">\n","> \\`\\`\\`python\n",">\n","> import numpy as np\n",">\n","> class Neuron:\n",">\n","> def \\_\\_init\\_\\_(self, input_size):\n",">\n","> \\# Initialize weights randomly\n",">\n","> self.weights = np.random.randn(input_size)\n",">\n","> \\# Initialize bias randomly\n",">\n","> self.bias = np.random.randn()\n",">\n","> def activate(self, x):\n",">\n","> \\# Calculate the weighted sum of inputs\n",">\n","> weighted_sum = np.dot(self.weights, x) + self.bias\n",">\n","> \\# Apply activation function (sigmoid in this case)\n",">\n","> activation = 1 / (1 + np.exp(-weighted_sum))\n",">\n","> return activation\n",">\n","> \\# Example usage\n",">\n","> input_data = np.array(\\[0.5, 0.3, 0.8\\]) \\# Input values\n",">\n","> neuron = Neuron(input_size=3) \\# Create a neuron with 3 inputs\n",">\n","> output = neuron.activate(input_data) \\# Activate the neuron\n",">\n","> print(output)\n",">\n","> \\`\\`\\`\n",">\n","> In this code, we define a \\`Neuron\\` class with an \\`\\_\\_init\\_\\_\\`\n","> method that initializes the neuron's weights and bias randomly. The\n","> \\`activate\\` method takes an input vector \\`x\\`, calculates the\n","> weighted sum of inputs, applies the sigmoid activation function, and\n","> returns the activation value.\n",">\n","> To use the neuron, we create an instance of the \\`Neuron\\` class and\n","> provide the input data as a numpy array. The \\`activate\\` method is\n","> called with the input data, and the output of the neuron is printed.\n","> Note that the size of the input vector (\\`input_size\\`) should match\n","> the size of the weights vector.\n","\n","Q2.  **Write the Python code to implement ReLU.**\n","\n","> Certainly! **Here's an example of Python code that implements the\n","> Rectified Linear Unit (ReLU) activation function:**\n",">\n","> \\`\\`\\`python\n",">\n","> import numpy as np\n",">\n","> def relu(x):\n",">\n","> return np.maximum(0, x)\n",">\n","> \\# Example usage\n",">\n","> input_data = np.array(\\[-2, -1, 0, 1, 2\\]) \\# Input values\n",">\n","> output = relu(input_data) \\# Apply ReLU activation\n",">\n","> print(output)\n",">\n","> \\`\\`\\`\n",">\n","> In this code, we define a function \\`relu\\` that takes an input \\`x\\`\n","> and applies the ReLU activation function element-wise using\n","> \\`np.maximum(0, x)\\`. The \\`np.maximum\\` function compares each\n","> element of \\`x\\` with 0 and returns the maximum value between 0 and\n","> the corresponding element.\n",">\n","> To use the ReLU function, we create an example input data as a numpy\n","> array (\\`input_data\\`), and then call the \\`relu\\` function with the\n","> input data. The output of the ReLU activation is stored in the\n","> \\`output\\` variable and printed. Note that the ReLU function is\n","> applied element-wise, so the output will have the same shape as the\n","> input.\n","\n","Q3.  **Write the Python code for a dense layer in terms of matrix\n","    multiplication.**\n","\n","> Certainly! **Here's an example of Python code that implements a dense\n","> layer using matrix multiplication:**\n",">\n","> \\`\\`\\`python\n",">\n","> import numpy as np\n",">\n","> class DenseLayer:\n",">\n","> def \\_\\_init\\_\\_(self, input_size, output_size):\n",">\n","> \\# Initialize weights randomly\n",">\n","> self.weights = np.random.randn(input_size, output_size)\n",">\n","> \\# Initialize bias randomly\n",">\n","> self.bias = np.random.randn(output_size)\n",">\n","> def forward(self, x):\n",">\n","> \\# Perform matrix multiplication between input and weights\n",">\n","> weighted_sum = np.dot(x, self.weights) + self.bias\n",">\n","> return weighted_sum\n",">\n","> \\# Example usage\n",">\n","> input_data = np.array(\\[\\[1, 2, 3\\]\\]) \\# Input values\n",">\n","> dense_layer = DenseLayer(input_size=3, output_size=2) \\# Create a\n","> dense layer with 3 inputs and 2 outputs\n",">\n","> output = dense_layer.forward(input_data) \\# Perform forward pass\n",">\n","> print(output)\n",">\n","> \\`\\`\\`\n",">\n","> In this code, we define a \\`DenseLayer\\` class with an\n","> \\`\\_\\_init\\_\\_\\` method that initializes the layer's weights and bias\n","> randomly. The \\`forward\\` method takes an input matrix \\`x\\` and\n","> performs matrix multiplication between \\`x\\` and the weights matrix,\n","> and then adds the bias vector.\n",">\n","> To use the dense layer, we create an instance of the \\`DenseLayer\\`\n","> class and provide the input data as a numpy array. The \\`forward\\`\n","> method is called with the input data, and the output of the dense\n","> layer is stored in the \\`output\\` variable and printed. Note that the\n","> input data should have the shape \\`(batch_size, input_size)\\`, and the\n","> output will have the shape \\`(batch_size, output_size)\\`, where\n","> \\`batch_size\\` is the number of input examples and \\`input_size\\` and\n","> \\`output_size\\` are the dimensions of the input and output vectors,\n","> respectively.\n","\n","Q4.  **Write the Python code for a dense layer in plain Python (that is,\n","    with list comprehensions and functionality built into Python).**\n","\n","> Certainly! **Here's an example of Python code that implements a dense\n","> layer using list comprehensions and basic functionality built into\n","> Python:**\n",">\n","> \\`\\`\\`python\n",">\n","> import random\n",">\n","> class DenseLayer:\n",">\n","> def \\_\\_init\\_\\_(self, input_size, output_size):\n",">\n","> \\# Initialize weights randomly\n",">\n","> self.weights = \\[\\[random.random() for \\_ in range(output_size)\\] for\n","> \\_ in range(input_size)\\]\n",">\n","> \\# Initialize bias randomly\n",">\n","> self.bias = \\[random.random() for \\_ in range(output_size)\\]\n",">\n","> def forward(self, x):\n",">\n","> \\# Perform matrix multiplication between input and weights\n",">\n","> weighted_sum = \\[sum(xi \\* wi for xi, wi in zip(x, w)) for w in\n","> self.weights\\]\n",">\n","> \\# Add bias to the weighted sum\n",">\n","> output = \\[sum(xi + bi for xi, bi in zip(weighted_sum, self.bias))\\]\n",">\n","> return output\n",">\n","> \\# Example usage\n",">\n","> input_data = \\[1, 2, 3\\] \\# Input values\n",">\n","> dense_layer = DenseLayer(input_size=3, output_size=2) \\# Create a\n","> dense layer with 3 inputs and 2 outputs\n",">\n","> output = dense_layer.forward(input_data) \\# Perform forward pass\n",">\n","> print(output)\n",">\n","> \\`\\`\\`\n",">\n","> In this code, we define a \\`DenseLayer\\` class with an\n","> \\`\\_\\_init\\_\\_\\` method that initializes the layer's weights and bias\n","> randomly using list comprehensions and the \\`random.random()\\`\n","> function. The \\`forward\\` method takes an input list \\`x\\` and\n","> performs matrix multiplication using list comprehensions and basic\n","> functionality built into Python. The weighted sum is computed by\n","> iterating over the rows of the weights matrix and using the \\`zip\\`\n","> function to multiply corresponding elements of \\`x\\` and the current\n","> row of weights. Finally, the bias is added to the weighted sum, and\n","> the output is returned as a list.\n",">\n","> To use the dense layer, we create an instance of the \\`DenseLayer\\`\n","> class and provide the input data as a list. The \\`forward\\` method is\n","> called with the input data, and the output of the dense layer is\n","> stored in the \\`output\\` variable and printed. Note that the input\n","> data should have the same length as the input size of the dense layer,\n","> and the output will be a list of length equal to the output size of\n","> the dense layer.\n","\n","Q5.  **What is the “hidden size” of a layer?**\n","\n","> The \"hidden size\" of a layer refers to the number of nodes or neurons\n","> present in that layer. In a neural network, a layer is composed of\n","> multiple nodes or neurons, and the hidden size indicates the number of\n","> these nodes.\n",">\n","> In a typical feedforward neural network, the layers are organized in a\n","> sequential manner, with an input layer, one or more hidden layers, and\n","> an output layer. Each layer, including the hidden layers, consists of\n","> a specific number of nodes or neurons. The hidden size determines the\n","> capacity or complexity of the layer and can have a significant impact\n","> on the network's ability to learn and represent complex patterns in\n","> the data.\n",">\n","> The choice of hidden size is a design decision that depends on various\n","> factors, including the complexity of the problem, the amount of\n","> available data, and computational resources. Increasing the hidden\n","> size can potentially allow the network to learn more complex\n","> relationships but may also increase the risk of overfitting and\n","> require more computational resources. On the other hand, using a\n","> smaller hidden size may lead to underfitting and limited learning\n","> capacity.\n","\n","Q6.  **What does the t method do in PyTorch?**\n","\n","> In PyTorch, the \\`t\\` method is used to transpose a tensor.\n","> Transposing a tensor swaps its dimensions, flipping the rows and\n","> columns.\n",">\n","> **Here's an example to demonstrate the usage of \\`t\\` in PyTorch:**\n",">\n","> \\`\\`\\`python\n",">\n","> import torch\n",">\n","> x = torch.tensor(\\[\\[1, 2, 3\\],\n",">\n","> \\[4, 5, 6\\]\\])\n",">\n","> x_transposed = x.t()\n",">\n","> print(x_transposed)\n",">\n","> \\`\\`\\`\n",">\n","> Output:\n",">\n","> \\`\\`\\`\n",">\n","> tensor(\\[\\[1, 4\\],\n",">\n","> \\[2, 5\\],\n",">\n","> \\[3, 6\\]\\])\n",">\n","> \\`\\`\\`\n",">\n","> In this example, we create a 2D tensor \\`x\\` with shape (2, 3). By\n","> calling \\`t\\` on \\`x\\`, we obtain a transposed tensor \\`x_transposed\\`\n","> with shape (3, 2). The rows of \\`x\\` become the columns of\n","> \\`x_transposed\\`, and vice versa.\n",">\n","> Note that the \\`t\\` method creates a new tensor with the transposed\n","> dimensions. It does not modify the original tensor in-place.\n","\n","Q7.  **Why is matrix multiplication written in plain Python very slow?**\n","\n","> Matrix multiplication implemented in plain Python using nested loops\n","> can be **slow for several reasons:**\n",">\n","> **1. Interpreted Execution:** Python is an interpreted language, which\n","> means that the code is executed line by line. This introduces\n","> additional overhead compared to compiled languages, where the entire\n","> code is pre-compiled into machine code. Consequently, interpreted\n","> execution slows down matrix multiplication when dealing with large\n","> matrices.\n",">\n","> **2. Lack of Vectorization:** Plain Python lacks built-in support for\n","> efficient vectorized operations. In matrix multiplication,\n","> vectorization allows operations to be performed on entire arrays or\n","> matrices rather than looping over individual elements. This vectorized\n","> approach takes advantage of lower-level optimizations and\n","> hardware-specific features, resulting in significantly faster\n","> computation.\n",">\n","> **3. No Multithreading or Parallelism:** The plain Python\n","> implementation of matrix multiplication typically executes\n","> sequentially on a single thread. It cannot take advantage of multiple\n","> cores or parallel execution, limiting performance. On the other hand,\n","> optimized libraries like NumPy or frameworks like PyTorch utilize\n","> efficient algorithms and parallelization techniques to accelerate\n","> matrix operations.\n",">\n","> **4. Data Type Checking and Dynamic Dispatch:** In Python, variables\n","> are dynamically typed, meaning their types can change at runtime. This\n","> dynamic typing, coupled with dynamic dispatch, introduces additional\n","> overhead for type checking and function dispatching. In contrast,\n","> compiled languages perform static type checking and optimization,\n","> resulting in faster execution.\n",">\n","> To overcome these limitations and improve performance, it is\n","> recommended to utilize optimized libraries like NumPy or frameworks\n","> like PyTorch, which leverage efficient underlying implementations,\n","> such as linear algebra libraries or GPU acceleration. These libraries\n","> employ optimized algorithms, utilize vectorized operations, and\n","> provide parallel execution to achieve significantly faster matrix\n","> multiplication compared to plain Python implementations.\n","\n","Q8.  **In matmul, why is ac==br?**\n","\n","> In matrix multiplication (often represented as \\`matmul\\` or \\`@\\`\n","> operator), the dimensions of the matrices involved determine whether\n","> the multiplication is valid and, if so, the shape of the resulting\n","> matrix. Specifically, for two matrices A and B to be multiplied, the\n","> number of columns in A must be equal to the number of rows in B.\n",">\n","> In the equation \\`C = A @ B\\`, where A is of shape (a, b) and B is of\n","> shape (c, d), the resulting matrix C will have a shape of (a, d). The\n","> number of rows in A (a) and the number of columns in B (d) determine\n","> the shape of the resulting matrix C.\n",">\n","> **To understand why \\`ac == br\\` is required in matrix multiplication,\n","> let's break it down:**\n",">\n","> \\- In matrix A, the number of rows is denoted as \\`a\\`, and the number\n","> of columns is denoted as \\`b\\`.\n",">\n","> \\- In matrix B, the number of rows is denoted as \\`c\\`, and the number\n","> of columns is denoted as \\`d\\`.\n",">\n","> For matrix multiplication \\`C = A @ B\\` to be valid, the number of\n","> columns in A (b) must be equal to the number of rows in B (c). In\n","> other words, \\`b\\` should be equal to \\`c\\`. This condition ensures\n","> that each element in A's rows can be multiplied with the corresponding\n","> element in B's columns, resulting in a valid matrix product.\n",">\n","> **Therefore,** in the equation \\`C = A @ B\\`, the condition \\`ac ==\n","> br\\` ensures that the matrices A and B can be multiplied, and the\n","> resulting matrix C will have the correct shape.\n","\n","Q9.  **In Jupyter Notebook, how do you measure the time taken for a\n","    single cell to execute?**\n","\n","> In Jupyter Notebook, you can measure the time taken for a single cell\n","> to execute using the \\`%timeit\\` magic command or the \\`%%timeit\\`\n","> cell magic command. These commands allow you to measure the execution\n","> time of a single line or an entire cell, respectively.\n",">\n","> **Here's how you can use \\`%timeit\\` and \\`%%timeit\\` in Jupyter\n","> Notebook:**\n",">\n","> **1. \\`%timeit\\` for a single line:**\n",">\n","> \\`\\`\\`python\n",">\n","> %timeit -r 1 -n 1 \\<your_code_here>\n",">\n","> \\`\\`\\`\n",">\n","> Replace \\`\\<your_code_here>\\` with the code you want to measure. \\`-r\n","> 1\\` specifies that the code should be executed only once, and \\`-n 1\\`\n","> specifies that timing should be performed only once.\n",">\n","> **2. \\`%%timeit\\` for an entire cell:**\n",">\n","> \\`\\`\\`python\n",">\n","> %%timeit -r 1 -n 1\n",">\n","> \\<your_code_here>\n",">\n","> \\`\\`\\`\n",">\n","> Replace \\`\\<your_code_here>\\` with the code block you want to measure.\n","> \\`-r 1\\` specifies that the code should be executed only once, and\n","> \\`-n 1\\` specifies that timing should be performed only once.\n",">\n","> After running the cell with \\`%timeit\\` or \\`%%timeit\\`, Jupyter\n","> Notebook will display the elapsed time for the code execution. It will\n","> also provide the average time taken per loop iteration if the code is\n","> executed multiple times.\n",">\n","> Note that using \\`-r\\` and \\`-n\\` with values greater than 1 can be\n","> useful for obtaining more accurate timing results by averaging over\n","> multiple runs. However, it will increase the overall execution time of\n","> the cell. Adjust these values according to your specific needs.\n",">\n","> Using \\`%timeit\\` or \\`%%timeit\\` allows you to quickly measure and\n","> compare the execution times of different code snippets or evaluate the\n","> performance of specific operations within your notebook.\n","\n","Q9.  **What is elementwise arithmetic?**\n","\n","> Elementwise arithmetic refers to performing arithmetic operations on\n","> corresponding elements of two or more arrays or vectors. In this\n","> context, each element of one array is combined with the corresponding\n","> element(s) from the other array(s) to produce a new array with the\n","> same shape.\n",">\n","> **For example, consider two arrays \\`A\\` and \\`B\\`:**\n",">\n","> \\`\\`\\`python\n",">\n","> A = \\[1, 2, 3\\]\n",">\n","> B = \\[4, 5, 6\\]\n",">\n","> \\`\\`\\`\n",">\n","> **Elementwise addition of \\`A\\` and \\`B\\` would result in:**\n",">\n","> \\`\\`\\`python\n",">\n","> A + B = \\[1+4, 2+5, 3+6\\] = \\[5, 7, 9\\]\n",">\n","> \\`\\`\\`\n",">\n","> Similarly, elementwise subtraction, multiplication, and division can\n","> be performed by applying the corresponding operations to each pair of\n","> corresponding elements.\n",">\n","> Elementwise arithmetic is a fundamental operation in many numerical\n","> and scientific computations, especially when dealing with arrays or\n","> matrices. It allows for efficient and concise operations on arrays\n","> without the need for explicit looping over individual elements.\n",">\n","> In programming languages and libraries that support vectorized\n","> operations, such as NumPy or PyTorch, elementwise arithmetic can be\n","> performed directly on arrays or tensors, often using operators like\n","> \\`+\\`, \\`-\\`, \\`\\*\\`, and \\`/\\`, or specific functions provided by the\n","> library. These vectorized operations leverage optimized\n","> implementations and can significantly improve performance compared to\n","> explicit looping in plain Python.\n","\n","Q10.  **Write the PyTorch code to test whether every element of a is\n","    greater than the corresponding element of b.**\n","\n","> Sure! **Here's an example PyTorch code to test whether every element\n","> of tensor \\`a\\` is greater than the corresponding element of tensor\n","> \\`b\\`:**\n",">\n","> \\`\\`\\`python\n",">\n","> import torch\n",">\n","> a = torch.tensor(\\[1, 2, 3\\])\n",">\n","> b = torch.tensor(\\[0, 2, 2\\])\n",">\n","> result = torch.all(a \\> b)\n",">\n","> print(result)\n",">\n","> \\`\\`\\`\n",">\n","> In this code, we create two PyTorch tensors \\`a\\` and \\`b\\` with the\n","> same shape. The \\`\\>\\` operator performs elementwise comparison\n","> between the elements of \\`a\\` and \\`b\\`, resulting in a boolean tensor\n","> with the same shape.\n",">\n","> The \\`torch.all\\` function is then used to check if all elements of\n","> the resulting tensor are \\`True\\`. If all elements are indeed greater\n","> in \\`a\\` compared to \\`b\\`, the \\`result\\` variable will be \\`True\\`.\n","> Otherwise, it will be \\`False\\`.\n",">\n","> Finally, we print the value of \\`result\\` to verify if every element\n","> of \\`a\\` is greater than the corresponding element of \\`b\\`.\n","\n","Q11.  **What is a rank-0 tensor? How do you convert it to a plain Python\n","    data type?**\n","\n","> In PyTorch, a rank-0 tensor refers to a scalar, which is a tensor with\n","> zero dimensions. It represents a single value, such as a single\n","> number, without any additional dimensions or shape.\n",">\n","> To convert a rank-0 tensor to a plain Python data type, you can use\n","> the \\`.item()\\` method. This method extracts the scalar value from the\n","> tensor and returns it as a native Python data type.\n",">\n","> **Here's an example:**\n",">\n","> \\`\\`\\`python\n",">\n","> import torch\n",">\n","> tensor = torch.tensor(42) \\# Creating a rank-0 tensor (scalar) with\n","> the value 42\n",">\n","> value = tensor.item() \\# Converting rank-0 tensor to a plain Python\n","> data type\n",">\n","> print(value) \\# Output: 42\n",">\n","> print(type(value)) \\# Output: \\<class 'int'>\n",">\n","> \\`\\`\\`\n",">\n","> **In this example,** we create a rank-0 tensor \\`tensor\\` with the\n","> value 42. By calling \\`.item()\\` on the tensor, we extract the scalar\n","> value and assign it to the variable \\`value\\`. The variable \\`value\\`\n","> now holds the scalar value as a plain Python \\`int\\` data type.\n",">\n","> Note that the \\`.item()\\` method works only for rank-0 tensors\n","> (scalars). If you try to use it on tensors with higher dimensions, it\n","> will raise an error.\n","\n","Q12.  **How does elementwise arithmetic help us speed up matmul?**\n","\n","> Elementwise arithmetic itself does not directly speed up the matrix\n","> multiplication (\\`matmul\\`) operation. Elementwise arithmetic is\n","> typically a component of more efficient implementations of matrix\n","> multiplication algorithms, rather than a standalone technique for\n","> speeding up the operation.\n",">\n","> Matrix multiplication involves performing a series of dot products\n","> between rows and columns of the input matrices. The dot product\n","> operation itself involves elementwise multiplication and subsequent\n","> summation. Elementwise arithmetic is used to perform these elementwise\n","> multiplications efficiently.\n",">\n","> **However, to speed up matrix multiplication, additional techniques\n","> are typically employed, such as:**\n",">\n","> **1. Vectorization:** Modern numerical computation libraries like\n","> NumPy or frameworks like PyTorch utilize vectorized operations, which\n","> can leverage hardware-specific optimizations and take advantage of\n","> lower-level instructions. Vectorized operations perform elementwise\n","> arithmetic efficiently by executing the operations on entire arrays or\n","> matrices at once, rather than looping over individual elements. This\n","> significantly speeds up the computation.\n",">\n","> **2. Parallelism:** Some matrix multiplication implementations utilize\n","> parallel computing techniques, such as multi-threading or GPU\n","> acceleration, to perform computations concurrently. These\n","> parallelization techniques distribute the workload across multiple\n","> cores or utilize the highly parallel nature of GPUs, enabling faster\n","> execution.\n",">\n","> **3. Optimized algorithms:** Advanced matrix multiplication\n","> algorithms, such as Strassen's algorithm or the Fast Fourier Transform\n","> (FFT)-based methods, are used to reduce the computational complexity\n","> of matrix multiplication. These algorithms exploit certain\n","> mathematical properties and optimize the number of operations\n","> required, resulting in faster execution.\n",">\n","> By combining efficient elementwise arithmetic with vectorization,\n","> parallelism, and optimized algorithms, implementations of matrix\n","> multiplication can achieve significant speed improvements over simple\n","> nested loops or plain Python implementations.\n","\n","Q13.  **What are the broadcasting rules?**\n","\n","> Broadcasting is a concept in NumPy and other numerical computation\n","> libraries that allows arrays with different shapes to be used together\n","> in elementwise operations. When performing elementwise operations,\n","> broadcasting automatically adjusts the shapes of arrays to make them\n","> compatible, avoiding the need for explicit copying or reshaping of\n","> arrays.\n",">\n","> The broadcasting rules define how arrays with different shapes are\n","> aligned and expanded to perform elementwise operations. These rules\n","> apply when operating on arrays with mismatched dimensions and\n","> determine how the arrays are broadcasted to achieve compatible shapes.\n",">\n","> **Here are the broadcasting rules in NumPy:**\n",">\n","> **1. Rule 1:** Scalar to Array: If one operand is a scalar (rank-0\n","> array), it is broadcasted to match the shape of the other operand.\n",">\n","> **2. Rule 2:** Size Compatibility: The arrays' shapes are compared\n","> elementwise starting from the trailing dimensions and moving towards\n","> the leading dimensions. Two dimensions are considered compatible if\n","> they are equal or one of them is 1. If the sizes are incompatible and\n","> not equal to 1, a ValueError is raised.\n",">\n","> **3. Rule 3:** Dimension Expansion: If the arrays have different\n","> numbers of dimensions, the array with fewer dimensions is padded with\n","> dimensions of size 1 on its left until the numbers of dimensions\n","> match.\n",">\n","> **4. Rule 4:** Array Copying: If any dimension has size greater than 1\n","> and is not equal to the corresponding dimension in the other array,\n","> the array with size 1 in that dimension is copied along that dimension\n","> to match the size of the other array.\n",">\n","> The broadcasting rules allow for more concise and efficient code by\n","> eliminating the need to explicitly reshape or duplicate arrays before\n","> performing elementwise operations. It enables operations between\n","> arrays of different shapes, as long as they can be aligned following\n","> the broadcasting rules.\n",">\n","> Broadcasting is a powerful tool that simplifies computations involving\n","> arrays of different shapes, enabling more flexible and efficient\n","> numerical operations.\n","\n","Q14.  **What is expand_as? Show an example of how it can be used to match\n","    the results of broadcasting.**\n","\n","> In PyTorch, the \\`expand_as\\` method is used to expand the size of a\n","> tensor to match the size of another tensor. It allows for aligning the\n","> dimensions of two tensors to enable elementwise operations or other\n","> computations.\n",">\n","> Here's **an example to illustrate how \\`expand_as\\` can be used to\n","> match the results of broadcasting:**\n",">\n","> \\`\\`\\`python\n",">\n","> import torch\n",">\n","> a = torch.tensor(\\[\\[1, 2, 3\\]\\])\n",">\n","> b = torch.tensor(\\[\\[4\\],\n",">\n","> \\[5\\]\\])\n",">\n","> expanded_a = a.expand_as(b)\n",">\n","> expanded_b = b.expand_as(a)\n",">\n","> print(\"Expanded a:\")\n",">\n","> print(expanded_a)\n",">\n","> print(\"Expanded b:\")\n",">\n","> print(expanded_b)\n",">\n","> \\`\\`\\`\n",">\n","> Output:\n",">\n","> \\`\\`\\`\n",">\n","> Expanded a:\n",">\n","> tensor(\\[\\[1, 2, 3\\],\n",">\n","> \\[1, 2, 3\\]\\])\n",">\n","> Expanded b:\n",">\n","> tensor(\\[\\[4, 4, 4\\],\n",">\n","> \\[5, 5, 5\\]\\])\n",">\n","> \\`\\`\\`\n",">\n","> **In this example,** we have tensors \\`a\\` and \\`b\\` with different\n","> shapes. \\`a\\` is of shape (1, 3), and \\`b\\` is of shape (2, 1). We\n","> want to perform elementwise operations between these tensors by\n","> aligning their dimensions.\n",">\n","> Using the \\`expand_as\\` method, we expand tensor \\`a\\` to match the\n","> shape of tensor \\`b\\`, resulting in \\`expanded_a\\`. Similarly, we\n","> expand tensor \\`b\\` to match the shape of tensor \\`a\\`, resulting in\n","> \\`expanded_b\\`. Both expanded tensors now have the same shape,\n","> allowing for elementwise operations or other computations.\n",">\n","> The \\`expand_as\\` method expands the tensor by replicating its data\n","> along the specified dimensions to match the shape of the input tensor.\n","> It does not create copies of the underlying data, but rather shares\n","> the memory between the original tensor and the expanded tensor."],"id":"A4NJsgr6LLTu"}],"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"provenance":[]}}}